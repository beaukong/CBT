{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成：获取嵌入\n"
     ]
    }
   ],
   "source": [
    "#---------------------获取线元嵌入-------------------------------\n",
    "from gettext import npgettext\n",
    "from msilib import sequence\n",
    "from transformers import BertForMaskedLM, BertTokenizer,BertModel  \n",
    "import torch,os  \n",
    "import numpy as np\n",
    "# 设置要使用的GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 显式指定使用哪个GPU\n",
    "torch.cuda.set_device(0)  \n",
    "# 加载预训练的BERT模型和分词器  \n",
    "ppath = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\data\\custom_tokenizer0318\\vocabContourW0512.txt'\n",
    "model_path = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\result\\Cps30epoch40Len128LR1e-4Bt256warm1000_H512L8A8\\checkpoint-63000'  # 你的模型保存路径  \n",
    "#待输入到BERT的句子txt\n",
    "input_Sequence_txtpath = r'G:\\BT\\data\\3downStreamTask\\LandformUnit\\sequenceCorpusJoinJXK.txt'\n",
    "#所需要获取嵌入的线元（TagID、以及在上边句子txt的行列位置）信息\n",
    "input_LineUnitPositionInSequence_txtpath = r'G:\\BT\\data\\3downStreamTask\\LandformUnit\\LineUnitPositionInSequence.txt'\n",
    "#输出线元TagID及其嵌入\n",
    "outTagID_Embedding_txtpath=r'G:\\BT\\data\\3downStreamTask\\LandformUnit\\TagID_Embedding.txt'\n",
    "#输出线元TagID及其word\n",
    "outTagID_word_txtpath=r'G:\\BT\\data\\3downStreamTask\\LandformUnit\\TagID_word.txt'\n",
    "\n",
    "LineUnitPositionInSequence_dic={}#存储各行的各个 线元TagID的位置\n",
    "LineUnitPositionIn_1Sequence_dic={}#存储一行中各个 线元TagID的位置\n",
    "outTagID_Embedding_dic={}#存储每个TagID及其嵌入\n",
    "#用于记录：\n",
    "outTagID_word_dic={}#存储每个TagID及其对应序列的word\n",
    "positionInMax_len=[]#存储目标线元在序列的第127各位置 的那些行\n",
    "positionInMin_len=[]#存储目标线元在序列的开头位置 的那些行\n",
    "max_len=128\n",
    "\n",
    "#加载待获取嵌入的线元信息\n",
    "input_LineUnitPositionInSequence=np.loadtxt(input_LineUnitPositionInSequence_txtpath,dtype=str,delimiter=' ')\n",
    "for i in range(len(input_LineUnitPositionInSequence)):  \n",
    "    tagID,row,col=input_LineUnitPositionInSequence[i]\n",
    "    row,col=int(row),int(col)\n",
    "    if col==(max_len-1) or col==(max_len-2):\n",
    "        positionInMax_len.append(row)\n",
    "    if col==0 or col==1:\n",
    "        positionInMin_len.append(row)\n",
    "    if row not in LineUnitPositionInSequence_dic:  \n",
    "        LineUnitPositionIn_1Sequence_dic={}\n",
    "        LineUnitPositionIn_1Sequence_dic[tagID]=col\n",
    "        LineUnitPositionInSequence_dic[row]=LineUnitPositionIn_1Sequence_dic\n",
    "    else:\n",
    "        LineUnitPositionIn_1Sequence_dic[tagID]=col\n",
    "        LineUnitPositionInSequence_dic[row]=LineUnitPositionIn_1Sequence_dic\n",
    "        \n",
    "# 加载tokenizer（分词器）  \n",
    "# tokenizer = BertTokenizer.from_pretrained(ppath)    #加载预训练模型的词汇表\n",
    "tokenizer = BertTokenizer(vocab_file=ppath, unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", mask_token=\"[MASK]\")\n",
    "tokenizer.model_max_length = max_len\n",
    "# 加载模型  \n",
    "model = BertForMaskedLM.from_pretrained(model_path,output_hidden_states=True)  \n",
    "  \n",
    "# 如果有GPU，则将模型移动到GPU上  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "model.to(device)  \n",
    "  \n",
    "# 遍历待输入到BERT的句子txt\n",
    "row_num=0\n",
    "with open(input_Sequence_txtpath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        isInMax_lenPosition=False\n",
    "        sequence = line.strip('\\n')\n",
    "        tokens=tokenizer.tokenize(sequence)\n",
    "        if len(tokens)>(max_len-2):  # 如果句子长度超过最大长度，则截断\n",
    "            if row_num in positionInMax_len:\n",
    "                isInMax_lenPosition=True\n",
    "                tokens=tokens[2:max_len]\n",
    "                # if row_num not in positionInMin_len:\n",
    "                #     tokens=tokens[2:max_len]\n",
    "                # else:#此时，该行的开头和结尾单词都在JXK内【未解决0520】\n",
    "            else:\n",
    "                tokens=tokens[:max_len-2]\n",
    "        # print(tokens)\n",
    "        tokens=['[CLS]']+tokens+['[SEP]']\n",
    "        # print(tokens)\n",
    "        attention_mask=[1 if i!='[PAD]' else 0 for i in tokens]\n",
    "        # print(attention_mask)\n",
    "        token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # print(token_ids)\n",
    "        token_ids=torch.tensor(token_ids).unsqueeze(0)\n",
    "        attention_mask=torch.tensor(attention_mask).unsqueeze(0)\n",
    "        token_ids = token_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # 获取模型的嵌入输出\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_ids, attention_mask=attention_mask)\n",
    "        hidden_rep = outputs.hidden_states\n",
    "        tokens_embedding=hidden_rep[-1][0]\n",
    "        LineUnitPositionIn_1Sequence_dic_temp=LineUnitPositionInSequence_dic[row_num]\n",
    "        if isInMax_lenPosition:#如果该行存在线元在第max_len位,tokens组成是:['[CLS]']+tokens[2:max_len-1]+['[SEP]']\n",
    "            for key,value in LineUnitPositionIn_1Sequence_dic_temp.items():\n",
    "                outTagID_Embedding_dic[key]=tokens_embedding[value-1]#减1是因为tagID在原始tokens的位置-1（如3-1）对应于目前tokens的位置（如2），例如原始tokens[token0,token1,token2],现在tokens[[Cls],token2]\n",
    "                outTagID_word_dic[key]=tokens[value-1]            \n",
    "        else:    #如果该行  不 存在线元在第max_len位,tokens组成是：['[CLS]']+tokens[:max_len-2]+['[SEP]']\n",
    "            for key,value in LineUnitPositionIn_1Sequence_dic_temp.items():\n",
    "                outTagID_Embedding_dic[key]=tokens_embedding[value+1]#加1是因为BERT中句子以[Cls]开头，0是[Cls].例如原始tokens[token0,token1,token2,...token127],现在tokens[[Cls],token0,token1,...token125]\n",
    "                outTagID_word_dic[key]=tokens[value+1]\n",
    "        row_num=row_num+1\n",
    "        # outTagID_Embedding_dic\n",
    "        # predic_token = outputs.logits\n",
    "        # print('hidden_rep[-1].shape')\n",
    "        # print(hidden_rep[-1].shape)#torch.Size([batchsize, sequenceLength,hidden_dims])最后一个隐藏层中各个词的嵌入\n",
    "        # print('hidden_rep[0]')\n",
    "        # print(hidden_rep[0])\n",
    "                \n",
    "        # print('predic_token.shape')\n",
    "        # print(predic_token.shape)#torch.Size([batchsize, sequenceLength, vocabSize]) 每个token被预测为词汇表中词的概率，未经softmax处理。\n",
    "        # print(predic_token[0][0])\n",
    "        # print(predic_token[0][1])\n",
    "#将 outTagID_Embedding_dic 写入到txt中outTagID_Embedding_txtpath\n",
    "with open (outTagID_Embedding_txtpath,'w') as file:\n",
    "    for key,value in outTagID_Embedding_dic.items():\n",
    "        # 将张量转为CPU上的numpy数组，以便更容易地转换为字符串\n",
    "        numpy_array = value.cpu().numpy()\n",
    "        # 使用numpy的savetxt方法将数组转换为字符串（这里我们直接构造字符串）\n",
    "        tensor_str = ' '.join(map(str, numpy_array.flatten().tolist()))\n",
    "        # 写入键和值（张量转换后的字符串形式）\n",
    "        file.write(f'{key}: {tensor_str}\\n')    \n",
    "with open (outTagID_word_txtpath,'w') as file:\n",
    "    for key,value in outTagID_word_dic.items():     \n",
    "        file.write(f'{key}: {value}\\n')\n",
    "print('完成：获取嵌入')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\4EmbeddingOfSequence\\LineUnit_平滑平缓型.txt 已成功删除。\n",
      "完成：获取嵌入G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\3Corpus126_0606\\LineUnit_平滑平缓型.txt\n",
      "文件 G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\4EmbeddingOfSequence\\LineUnit_平滑陡峭型.txt 已成功删除。\n",
      "完成：获取嵌入G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\3Corpus126_0606\\LineUnit_平滑陡峭型.txt\n",
      "文件 G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\4EmbeddingOfSequence\\LineUnit_曲折平缓型.txt 已成功删除。\n",
      "完成：获取嵌入G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\3Corpus126_0606\\LineUnit_曲折平缓型.txt\n",
      "文件 G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\4EmbeddingOfSequence\\LineUnit_曲折陡峭型.txt 已成功删除。\n",
      "完成：获取嵌入G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\3Corpus126_0606\\LineUnit_曲折陡峭型.txt\n"
     ]
    }
   ],
   "source": [
    "#---------------------获取序列嵌入-------------------------------\n",
    "from gettext import npgettext\n",
    "from msilib import sequence\n",
    "from transformers import BertForMaskedLM, BertTokenizer,BertModel  \n",
    "import torch,os  \n",
    "import numpy as np\n",
    "# 设置要使用的GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 显式指定使用哪个GPU\n",
    "torch.cuda.set_device(0)  \n",
    "# 加载预训练的BERT模型和分词器  \n",
    "ppath = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\data\\custom_tokenizer0318\\vocabContourW0512.txt'\n",
    "model_path = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\result\\Cps30epoch40Len128LR1e-4Bt256warm1000_H512L8A8\\checkpoint-63000'  # 你的模型保存路径  \n",
    "#待输入到BERT的句子txt\n",
    "input_Sequence_txtDiretory=r'G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\3Corpus126_0606'\n",
    "# input_Sequence_txtpath = r'G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\3Corpus126_0606\\LineUnit_平滑平缓型.txt'\n",
    "out_Embedding_directory=r'G:\\BT\\data\\3downStreamTask\\MorphologicalClustering\\4EmbeddingOfSequence'\n",
    "out_sequence_embedding_lst=[]\n",
    "\n",
    "# 加载tokenizer（分词器）  \n",
    "# tokenizer = BertTokenizer.from_pretrained(ppath)    #加载预训练模型的词汇表\n",
    "tokenizer = BertTokenizer(vocab_file=ppath, unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", mask_token=\"[MASK]\")\n",
    "tokenizer.model_max_length = max_len\n",
    "# 加载模型  \n",
    "model = BertForMaskedLM.from_pretrained(model_path,output_hidden_states=True)  \n",
    "  \n",
    "# 如果有GPU，则将模型移动到GPU上  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "model.to(device)  \n",
    "  \n",
    "# 遍历待输入到BERT的句子txt\n",
    "# 遍历目标目录及其所有子目录  \n",
    "for root, dirs, files in os.walk(input_Sequence_txtDiretory):  \n",
    "    for input_Sequence_txtName in files:  \n",
    "        if not input_Sequence_txtName.endswith(\".txt\"):\n",
    "            print(input_Sequence_txtName)\n",
    "            continue\n",
    "        out_sequence_embedding_lst=[]\n",
    "        input_Sequence_txtpath=os.path.join(input_Sequence_txtDiretory,input_Sequence_txtName)\n",
    "        with open(input_Sequence_txtpath, 'r', encoding='utf-8') as f:\n",
    "            out_Embedding_path=os.path.join(out_Embedding_directory,input_Sequence_txtName)\n",
    "            if os.path.exists(out_Embedding_path): \n",
    "                # 如果文件存在，则删除它  \n",
    "                try:  \n",
    "                    os.remove(out_Embedding_path)  \n",
    "                    print(f\"文件 {out_Embedding_path} 已成功删除。\")  \n",
    "                except OSError as e:  \n",
    "                    print(f\"删除文件 {out_Embedding_path} 时出错: {e.strerror}\")             \n",
    "            for line in f:\n",
    "                # isInMax_lenPosition=False\n",
    "                sequence = line.strip('\\n')\n",
    "                tokens=tokenizer.tokenize(sequence)\n",
    "                tokens=['[CLS]']+tokens+['[SEP]']\n",
    "                # print(tokens)\n",
    "                attention_mask=[1 if i!='[PAD]' else 0 for i in tokens]\n",
    "                # print(attention_mask)\n",
    "                token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "                # print(token_ids)\n",
    "                token_ids=torch.tensor(token_ids).unsqueeze(0)\n",
    "                attention_mask=torch.tensor(attention_mask).unsqueeze(0)\n",
    "                token_ids = token_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "\n",
    "                # 获取模型的嵌入输出\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(token_ids, attention_mask=attention_mask)\n",
    "                hidden_rep = outputs.hidden_states\n",
    "                tokens_embedding=hidden_rep[-1][0]\n",
    "                sequence_embedding=tokens_embedding[0].cpu().tolist()\n",
    "                out_sequence_embedding_lst.append(sequence_embedding)\n",
    "        \n",
    "        np.savetxt(out_Embedding_path,out_sequence_embedding_lst)  \n",
    "        print('完成：获取嵌入'+input_Sequence_txtpath)  \n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptll2py39cull3PYG",
   "language": "python",
   "name": "ptll2py39cull3pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
