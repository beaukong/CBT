{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\result\\Cps30epoch40Len128LR1e-4Bt256warm1000_H512L8A8\\checkpoint-63000 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch 1, Average Loss: 0.27340148850565865, Accuracy: 0.8851586138426626\n",
      "Saving model...\n",
      "Epoch 2/30\n",
      "Epoch 2, Average Loss: 0.21866507757277714, Accuracy: 0.9063445537065052\n",
      "Saving model...\n",
      "Epoch 3/30\n",
      "Epoch 3, Average Loss: 0.19325370261711733, Accuracy: 0.9207699035552194\n",
      "Saving model...\n",
      "Epoch 4/30\n",
      "Epoch 4, Average Loss: 0.17014953077194236, Accuracy: 0.9295574886535553\n",
      "Saving model...\n",
      "Epoch 5/30\n",
      "Epoch 5, Average Loss: 0.1587733184652669, Accuracy: 0.938557819591528\n",
      "Saving model...\n",
      "Epoch 6/30\n",
      "Epoch 6, Average Loss: 0.12243428611240927, Accuracy: 0.9532018248865356\n",
      "Saving model...\n",
      "Epoch 7/30\n",
      "Epoch 7, Average Loss: 0.09201033085229851, Accuracy: 0.963561365355522\n",
      "Saving model...\n",
      "Epoch 8/30\n",
      "Epoch 8, Average Loss: 0.06669601585183825, Accuracy: 0.9746832450832073\n",
      "Saving model...\n",
      "Epoch 9/30\n",
      "Epoch 9, Average Loss: 0.06336395209655166, Accuracy: 0.9757647031013615\n",
      "Saving model...\n",
      "Epoch 10/30\n",
      "Epoch 10, Average Loss: 0.057694194106651206, Accuracy: 0.9775257658850227\n",
      "Saving model...\n",
      "Epoch 11/30\n",
      "Epoch 11, Average Loss: 0.05257133568548376, Accuracy: 0.9800787159606656\n",
      "Saving model...\n",
      "Epoch 12/30\n",
      "Epoch 12, Average Loss: 0.052975384562852834, Accuracy: 0.9792986478819969\n",
      "Saving model...\n",
      "Epoch 13/30\n",
      "Epoch 13, Average Loss: 0.04128276512935935, Accuracy: 0.983937689107413\n",
      "Saving model...\n",
      "Epoch 14/30\n",
      "Epoch 14, Average Loss: 0.036323212375420896, Accuracy: 0.9870284133888049\n",
      "Saving model...\n",
      "Epoch 15/30\n",
      "Epoch 15, Average Loss: 0.017738676136581318, Accuracy: 0.9935467095310136\n",
      "Saving model...\n",
      "Epoch 16/30\n",
      "Epoch 16, Average Loss: 0.010526057461642526, Accuracy: 0.9963124054462935\n",
      "Saving model...\n",
      "Epoch 17/30\n",
      "Epoch 17, Average Loss: 0.008948355467596446, Accuracy: 0.996938823751891\n",
      "Saving model...\n",
      "Epoch 18/30\n",
      "Epoch 18, Average Loss: 0.006823000035226522, Accuracy: 0.99770116301059\n",
      "Saving model...\n",
      "Epoch 19/30\n",
      "Epoch 19, Average Loss: 0.006370816153364666, Accuracy: 0.9977425302571861\n",
      "Saving model...\n",
      "Epoch 20/30\n",
      "Epoch 20, Average Loss: 0.004745889652223836, Accuracy: 0.9983866773827534\n",
      "Saving model...\n",
      "Epoch 21/30\n",
      "Epoch 21, Average Loss: 0.004198820866371361, Accuracy: 0.9985107791225416\n",
      "Saving model...\n",
      "Epoch 22/30\n",
      "Epoch 22, Average Loss: 0.004588396345276297, Accuracy: 0.9982803044629349\n",
      "Saving model...\n",
      "Epoch 23/30\n",
      "Epoch 23, Average Loss: 0.0042605002492477765, Accuracy: 0.9986348808623298\n",
      "Saving model...\n",
      "Epoch 24/30\n",
      "Epoch 24, Average Loss: 0.004921662101228971, Accuracy: 0.998209389183056\n",
      "Saving model...\n",
      "Epoch 25/30\n",
      "Epoch 25, Average Loss: 0.0040041864517011815, Accuracy: 0.9986703385022693\n",
      "Saving model...\n",
      "Epoch 26/30\n",
      "Epoch 26, Average Loss: 0.003984805071792964, Accuracy: 0.9986053328290468\n",
      "Saving model...\n",
      "Epoch 27/30\n",
      "Epoch 27, Average Loss: 0.0029644945593428843, Accuracy: 0.9990367341149773\n",
      "Saving model...\n",
      "Epoch 28/30\n",
      "Epoch 28, Average Loss: 0.0027377241924460534, Accuracy: 0.9990603725416036\n",
      "Saving model...\n",
      "Epoch 29/30\n",
      "Epoch 29, Average Loss: 0.0032341417726933115, Accuracy: 0.9988949035552194\n",
      "Saving model...\n",
      "Epoch 30/30\n",
      "Epoch 30, Average Loss: 0.0024365055557739523, Accuracy: 0.9991726550680786\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------6.加载用于微调的数据-----------------------------------\n",
    "# 在加载数据集时传入 tokenizer\n",
    "ppath = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\data\\custom_tokenizer0318\\vocabContourW0512.txt'\n",
    "pretrained_model_path = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\result\\Cps30epoch40Len128LR1e-4Bt256warm1000_H512L8A8\\checkpoint-63000' \n",
    "#Corpus&Lablel_Classified_Split257_XianYang_LineUnit_13万多线元用Split257相交     Corpus&Lablel_Classified_Split257_XianYang_LineUnit_8万多线元用代相交.txt\n",
    "finTuneDataPath=r\"G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\3Courpus_LineUnit&TagID&Label\\Corpus&Lablel_Classified_Split257_XianYang_LineUnit_13万多线元用Split257相交.txt\"\n",
    "outFineTuneModelPath=r\"G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\4FineTunedModel&Result\\test\"#\"./dataTokenCls_SentencenSegmentation/result/fine_tuned_model\"  \n",
    "outResMetricPath=r\"G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\4FineTunedModel&Result\"\n",
    "batch_size=32\n",
    "max_len=128#最大句子长度\n",
    "num_labels = 2  # O: 普通token, SEP: 句子分割token\n",
    "num_epochs = 30\n",
    "save_eval_steps=100\n",
    "save_total_limits=100\n",
    "lr=5e-5\n",
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments,BertTokenizer,DataCollatorForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch,os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 禁用代理\n",
    "os.environ[\"HTTP_PROXY\"] = \"\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer(vocab_file=ppath, unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", mask_token=\"[MASK]\")\n",
    "tokenizer.model_max_length = max_len\n",
    "# tokenizer = BertTokenizer.from_pretrained(occDir)  # 或者BertTokenizer(occDir + '/vocab.txt')\n",
    "# text = \"urban function graph method\"\n",
    "# encoding = tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "# input_ids = encoding['input_ids']\n",
    "# attention_mask = encoding['attention_mask']\n",
    "# print(\"Input IDs:\", input_ids)\n",
    "# print(\"Attention Mask:\", attention_mask)\n",
    "\n",
    "class FineTuneDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer,max_len):\n",
    "        self.samples = []\n",
    "        # self.label_mapping = {0: \"O\", 1: \"SEP\"}  # 定义标签映射\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len  # 添加max_len作为类的属性         \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()  # \"4 Results\"\n",
    "                tokens_and_labels = line.split()  # 按空格split\n",
    "                tokens = tokens_and_labels[:-1]\n",
    "                labels = [int(label) for label in tokens_and_labels[-1].split(\",\")]  # 按， split\n",
    "\n",
    "                # 添加起始和结束标记否则会提示Using bos_token and eos_token, but it is not set yet. 是由于使用了预训练模型，而你的输入数据中没有设置起始标记（bos_token）和结束标记（eos_token）。这两个标记通常在训练时添加，但在推理时可能不需要。\n",
    "                tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]  \n",
    "                labels = [0] + labels + [0]  \n",
    "\n",
    "                # 检查句子长度并截断  \n",
    "                if len(tokens) > self.max_len:  \n",
    "                    tokens = tokens[:self.max_len-1]  # 保留CLS但去掉超出长度的部分和SEP  \n",
    "                    labels = labels[:self.max_len-1]  # 保留对应标签  \n",
    "                    tokens.append(self.tokenizer.sep_token)  # 最后添加SEP  \n",
    "                    labels.append(0)  # 添加对应SEP的标签  \n",
    "  \n",
    "                # 如果句子长度小于max_len，则进行填充  \n",
    "                padding_length = self.max_len - len(tokens)  \n",
    "                tokens += [self.tokenizer.pad_token] * padding_length  \n",
    "                labels += [0] * padding_length  \n",
    "  \n",
    "                self.samples.append((tokens, labels))              \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.samples[idx]\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.samples[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)),\n",
    "            \"attention_mask\": torch.tensor([1] * len(tokens)),  # 根据需求自定义 attention_mask\n",
    "            \"labels\": torch.tensor(labels),\n",
    "        }\n",
    "\n",
    "fine_tune_dataset = FineTuneDataset(finTuneDataPath, tokenizer,max_len)\n",
    "fine_tune_dataloader = DataLoader(fine_tune_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 自定义的数据collator\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding='max_length',max_length=max_len,label_pad_token_id=pad_token_id, pad_to_multiple_of=tokenizer.model_max_length)\n",
    "# truncation=True, max_length=max_len, return_tensors='pt'\n",
    "# ---------------------------------7.初始化用于微调的模型---------------------------------\n",
    "# 加载预训练的模型\n",
    "pretrained_model = BertForTokenClassification.from_pretrained(pretrained_model_path)\n",
    "\n",
    "# 根据微调数据的标签数量调整分类器的输出维度（这里假设标签为O和SEP）\n",
    "\n",
    "# 获取预训练模型的配置\n",
    "config = pretrained_model.config\n",
    "\n",
    "# 创建新的分类器，并设置正确的输入和输出维度\n",
    "pretrained_model.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "# 确保微调模型输出层的权重被初始化\n",
    "pretrained_model.classifier.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "pretrained_model.classifier.bias.data.zero_()\n",
    "\n",
    "# 设置微调 Trainer 参数\n",
    "training_args_fine_tune = TrainingArguments(\n",
    "    output_dir=outFineTuneModelPath,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,  # 调整微调轮数\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=save_eval_steps,\n",
    "    eval_steps=save_eval_steps,\n",
    "    save_total_limit=save_total_limits,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# 初始化微调 Trainer\n",
    "trainer_fine_tune = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args_fine_tune,\n",
    "    train_dataset=fine_tune_dataset,\n",
    "    data_collator=data_collator,  # 使用之前定义的 data_collator\n",
    ")\n",
    "# ---------------------------------8.开始微调，并保存微调后的模型--------------------------------\n",
    "optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=lr)  # 初始化AdamW优化器\n",
    "pretrained_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pretrained_model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    predictions = []\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch_idx, batch in enumerate(fine_tune_dataloader):\n",
    "        # 获取数据\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(torch.device(\"cuda\")),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(torch.device(\"cuda\")),\n",
    "            \"labels\": batch[\"labels\"].to(torch.device(\"cuda\")),\n",
    "        }\n",
    "\n",
    "        # 微调\n",
    "        outputs = pretrained_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 获取预测结果\n",
    "        logits = outputs.logits\n",
    "        predictions_batch = torch.argmax(logits, dim=num_labels)\n",
    "        predictions.extend(predictions_batch.cpu().numpy())\n",
    "\n",
    "        # 计算准确度\n",
    "        labels = inputs[\"labels\"]\n",
    "        correct_predictions += (predictions_batch == labels).sum().item()\n",
    "        total_samples += labels.numel()\n",
    "\n",
    "        # 更新权重\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # # 保存当前 batch 的预测结果\n",
    "        # batch_predictions_file = f\"./dataTokenCls_SentencenSegmentation/result/batch_predictions_epoch_{epoch + 1}_batch_{batch_idx + 1}.txt\"\n",
    "        # with open(batch_predictions_file, \"w\", encoding=\"utf-8\") as batch_file:\n",
    "        #     for token_predictions in predictions_batch.cpu().numpy():\n",
    "        #         token_predictions = [str(pred) for pred in token_predictions]\n",
    "        #         batch_file.write(\" \".join(token_predictions) + \"\\n\")\n",
    "\n",
    "    # 保存整个 epoch 的预测结果\n",
    "    epoch_predictions_file = f\"{outResMetricPath}epoch_predictions_epoch_{epoch + 1}.txt\"\n",
    "    with open(epoch_predictions_file, \"w\", encoding=\"utf-8\") as epoch_file:\n",
    "        for token_predictions in predictions:\n",
    "            token_predictions = [str(pred) for pred in token_predictions]\n",
    "            epoch_file.write(\" \".join(token_predictions) + \"\\n\")\n",
    "\n",
    "    # 计算平均损失和准确度\n",
    "    average_loss = total_loss / len(fine_tune_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    pretrained_model.save_pretrained(f\"{outFineTuneModelPath}fine_tuned_model_epoch_{epoch + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split255_BaoJi_LineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split255_BaoJi_LineUnit.txt  Average Loss: 0.1346, Accuracy: 0.98\n",
      "\n",
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split257_BaiYin_LineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split257_BaiYin_LineUnit.txt  Average Loss: 0.7429, Accuracy: 0.8906\n",
      "\n",
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split257_PingLiang_LineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split257_PingLiang_LineUnit.txt  Average Loss: 0.2494, Accuracy: 0.963\n",
      "\n",
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split257_QingYang_LineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split257_QingYang_LineUnit.txt  Average Loss: 0.2334, Accuracy: 0.9681\n",
      "\n",
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split257_TongChuan_DelLineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split257_TongChuan_DelLineUnit.txt  Average Loss: 0.3123, Accuracy: 0.9559\n",
      "\n",
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split257_WeiNan_DelLineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split257_WeiNan_DelLineUnit.txt  Average Loss: 0.4622, Accuracy: 0.9377\n",
      "\n",
      "文件 G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult\\PreLabelCorpus&Lablel_Classified_Split257_YunCheng_LineUnit.txt 已成功删除。\n",
      "PreLabelCorpus&Lablel_Classified_Split257_YunCheng_LineUnit.txt  Average Loss: 0.428, Accuracy: 0.9356\n",
      "\n",
      "PreLabelCorpus&Lablel_Classified_Split257_XianYang_LineUnit.txt  Average Loss: 0.2617, Accuracy: 0.9622\n",
      "\n",
      "完成测试\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------9.（1）预测测试数据文件-----------也可预测微调数据，进行分析---------------------\n",
    "import os,torch,itertools\n",
    "# 禁用代理\n",
    "os.environ[\"HTTP_PROXY\"] = \"\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"\"\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=32\n",
    "num_labels = 2  # O: 普通token, 1: 塬\n",
    "max_len=128#最大句子长度\n",
    "from sklearn.metrics import confusion_matrix\n",
    "fine_tuned_model_path = r\"G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\4FineTunedModel&Result\\test\\fine_tuned_model_epoch_30\"  # 替换成你保存微调后模型的路径\n",
    "ppath = r'G:\\BT\\program\\ContourBERT\\dataTokenCls_SentencenSegmentation\\data\\custom_tokenizer0318\\vocabContourW0512.txt'#词汇表\n",
    "outResMetricPath=r'G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\5TestResult'\n",
    "testDatasetDirectory=r\"G:\\BT\\data\\3downStreamTask\\SemanticSegmentation\\3Courpus_LineUnit&TagID&Label\"\n",
    "testData_lst=[\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split255_BaoJi_LineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_BaiYin_LineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_PingLiang_LineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_QingYang_LineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_TongChuan_DelLineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_WeiNan_DelLineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_YunCheng_LineUnit.txt'),\n",
    "    os.path.join(testDatasetDirectory,'Corpus&Lablel_Classified_Split257_XianYang_LineUnit.txt'),\n",
    "]\n",
    "# 定义待预测的数据集类\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.samples = []\n",
    "        # self.label_mapping = {0: \"O\", 1: \"SEP\"}  # 定义标签映射\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip()  # \"4 Results\"\n",
    "                tokens_and_labels = line.split()  # 按空格split\n",
    "                tokens = tokens_and_labels[:-1]\n",
    "                labels = [int(label) for label in tokens_and_labels[-1].split(\",\")]  # 按， split\n",
    "\n",
    "                # 添加起始和结束标记否则会提示Using bos_token and eos_token, but it is not set yet. 是由于使用了预训练模型，而你的输入数据中没有设置起始标记（bos_token）和结束标记（eos_token）。这两个标记通常在训练时添加，但在推理时可能不需要。\n",
    "                tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]\n",
    "                labels = [0] + labels + [0]\n",
    "\n",
    "                self.samples.append((tokens, labels))\n",
    "                # print((tokens, labels))\n",
    "        # 手动填充：\n",
    "        for i in range(len(self.samples)):\n",
    "            tokens, labels = self.samples[i]\n",
    "            padding_length = max_len - len(tokens)\n",
    "            tokens += [self.tokenizer.pad_token] * padding_length\n",
    "            labels += [0] * padding_length\n",
    "            self.samples[i] = (tokens, labels)                  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.samples[idx]\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.samples[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)),\n",
    "            \"attention_mask\": torch.tensor([1] * len(tokens)),  # 根据需求自定义 attention_mask\n",
    "            \"labels\": torch.tensor(labels),\n",
    "        }\n",
    "\n",
    "# 绘制混淆矩阵 显示百分比或数值\n",
    "def plot_confusion_matrix_percent(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues,confusionMatrixPngPath='./confusionMatrixPng.png'):\n",
    "    \"\"\"\n",
    "    - cm : 计算出的混淆矩阵的值\n",
    "    - classes : 混淆矩阵中每一行每一列对应的列\n",
    "    - normalize : True:显示百分比, False:显示个数\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        # print(\"混淆矩阵：\")\n",
    "        np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
    "        # print(cm)\n",
    "    else:\n",
    "        print()\n",
    "        # print('混淆矩阵：')\n",
    "        # print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    # matplotlib版本问题，如果不加下面这行代码，则绘制的混淆矩阵上下只能显示一半，有的版本的matplotlib不需要下面的代码，分别试一下即可\n",
    "    plt.ylim(len(classes) - 0.5, -0.5)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # plt.show()\n",
    "    plt.savefig(confusionMatrixPngPath)\n",
    "    plt.close()\n",
    "    \n",
    "# 1. 加载微调后的模型\n",
    "fine_tuned_model = BertForTokenClassification.from_pretrained(fine_tuned_model_path,output_hidden_states=True).to(device)\n",
    "# 2. 加载微调时使用的tokenizer\n",
    "tokenizer = BertTokenizer(vocab_file=ppath, unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", mask_token=\"[MASK]\")\n",
    "tokenizer.model_max_length = max_len\n",
    "for testDatasetPath in testData_lst:\n",
    "    outResultFileName=\"PreLabel\"+os.path.basename(testDatasetPath)\n",
    "    predictions_file=os.path.join(outResMetricPath, outResultFileName)\n",
    "    if os.path.exists(predictions_file): \n",
    "        # 如果文件存在，则删除它  \n",
    "        try:  \n",
    "            os.remove(predictions_file)  \n",
    "            print(f\"文件 {predictions_file} 已成功删除。\")  \n",
    "        except OSError as e:  \n",
    "            print(f\"删除文件 {predictions_file} 时出错: {e.strerror}\") \n",
    "\n",
    "    # 3. 加载测试数据集\n",
    "    test_dataset = TestDataset(testDatasetPath, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)#, collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
    "\n",
    "    # 4. 使用微调后的模型进行预测\n",
    "    fine_tuned_model.eval()\n",
    "    total_samples,total_loss,correct_predictions = 0,0,0\n",
    "    average_loss,accuracy=0,0\n",
    "    predictions,labels,predictions2 = [],[],[]\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": batch[\"labels\"].to(device),\n",
    "            }\n",
    "            outputs = fine_tuned_model(**inputs)\n",
    "            # predicted_labels = torch.argmax(outputs.logits, dim=2).tolist()\n",
    "            # predictions.extend(predicted_labels)\n",
    "        \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 获取预测结果\n",
    "            logits = outputs.logits\n",
    "            predictions_batch = torch.argmax(logits, dim=num_labels)\n",
    "\n",
    "            # 计算准确度\n",
    "            labels_batch = inputs[\"labels\"]\n",
    "            correct_predictions += (predictions_batch == labels_batch).sum().item()\n",
    "            total_samples += labels_batch.numel()  \n",
    "            \n",
    "            predictions_batch  =predictions_batch.cpu().numpy().tolist()   \n",
    "            predictions2+=predictions_batch\n",
    "            labels_batch=labels_batch.cpu().numpy().tolist()\n",
    "            for item_pre in predictions_batch:\n",
    "                predictions+=item_pre\n",
    "            for item_label in labels_batch:\n",
    "                labels+=item_label           \n",
    "                 \n",
    "        # 计算平均损失和准确度\n",
    "        average_loss =np.round(total_loss / len(test_dataloader),decimals=4)\n",
    "        accuracy = np.round(correct_predictions / total_samples,decimals=4)\n",
    "        print(outResultFileName+f\"  Average Loss: {average_loss}, Accuracy: {accuracy}\")       \n",
    "    # 5. 输出打印预测结果\n",
    "\n",
    "    # for i, pred in enumerate(predictions):\n",
    "    #     tokens = tokenizer.convert_ids_to_tokens(test_dataset[i][\"input_ids\"])\n",
    "    #     print(f\"Sample {i + 1} - Tokens: {tokens}\")\n",
    "    #     print(f\"Sample {i + 1} - Predicted Labels: {pred}\")\n",
    "    #     print()\n",
    "    with open(predictions_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for token_predictions in predictions2:\n",
    "            token_predictions = [str(pred) for pred in token_predictions]\n",
    "            file.write(\" \".join(token_predictions) + \"\\n\")\n",
    "    #混淆矩阵（测试数据的预测结果 ）\n",
    "    attack_types = ['other','Loess tablelannd']\n",
    "    cm_train= confusion_matrix(labels,predictions)\n",
    "    plot_confusion_matrix_percent(cm_train, classes=attack_types, normalize=False, title=f\"  Average Loss {average_loss}, Accuracy {accuracy}\",confusionMatrixPngPath=os.path.join(outResMetricPath,f\"  Average Loss {average_loss}, Accuracy {accuracy}\"+outResultFileName+\".png\"))\n",
    "             \n",
    "print('完成测试')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
